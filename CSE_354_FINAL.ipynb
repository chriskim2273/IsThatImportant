{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chriskim2273/IsThatImportant/blob/main/CSE_354_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is that Important?\n",
        "Antony Shen & Christopher Kim\n",
        "\n",
        "Project Overview:\n",
        "\n",
        "  Large language models are typically trained to predict the next token The overall goal of the project is to understand how much relevant information should be fed into large language models.\n",
        "\n",
        "\n",
        "\n",
        "--------------------------------------------\n",
        "**outline**\n",
        "\n",
        "~3k dev dataset\n",
        "use maybe 400 for each eval instance\n",
        "\n",
        "possibly create a fresh model trained on the test dataset to use as a baseline?\n",
        "(- doesn't align with overall project goal? -)\n",
        "\n",
        "working api calls\n",
        "\n",
        "Bart Large MNLI (can do zero shot yes/no)\n",
        "\n",
        "Flan T5 XXL (few shot only)\n",
        "\n",
        "GPT Neo 125m (few shot only)\n",
        "\n",
        "--------------------------------------------\n",
        "\n",
        "Bart - Zero Shot\n",
        "\n",
        "\n",
        "few shot (w/ title, question)\n",
        "\n",
        "few shot (w/ title, passage, question)\n",
        "\n",
        "for T5 and GPT Neo\n",
        "\n",
        "maybe less data \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZPeykz5Kh0FA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers\n",
        "#!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqZ7mk9KPqOC",
        "outputId": "e8ed19b9-4358-4a58-a5ee-23418d498812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbQ3e0DZNK3a"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "import json\n",
        "#from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "#import torch\n",
        "import requests\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xxl\")\n",
        "\n",
        "#model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-xxl\")\n",
        "\n",
        "#device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "#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\n",
        "headers = {\"Authorization\": \"Bearer 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\"}\n",
        "\n",
        "# Modified Methods from HuggingFace documentation to query each respective model.\n",
        "\n",
        "BART_URL = \"https://api-inference.huggingface.co/models/facebook/bart-large-mnli\"\n",
        "def BART_query(payload):\n",
        "    data = json.dumps(payload)\n",
        "    response = requests.request(\"POST\", BART_URL, headers=headers, data=data)\n",
        "    return json.loads(response.content.decode(\"utf-8\"))\n",
        "\n",
        "FLAN_URL = \"https://api-inference.huggingface.co/models/google/flan-t5-xxl\"\n",
        "\n",
        "def FLAN_query(payload):\n",
        "\tresponse = requests.post(FLAN_URL, headers=headers, json=payload)\n",
        "\treturn response.json()\n",
        "\n",
        "GPT_URL = \"https://api-inference.huggingface.co/models/EleutherAI/gpt-neo-125m\"\n",
        "def GPT_query(payload):\n",
        "\tresponse = requests.post(GPT_URL, headers=headers, json=payload)\n",
        "\treturn response.json()\n",
        "\t"
      ],
      "metadata": {
        "id": "sMYG0i41UhfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test methods (sometimes used to see if we hit rate limit lol)\n",
        "\n",
        "bart_output = BART_query(\n",
        "    {\n",
        "        \"inputs\": \"The Empire State Building in NYC is taller than Mount Everest.\",\n",
        "        \"parameters\": {\"candidate_labels\": [\"true\",\"false\"]},\n",
        "    }\n",
        ")\n",
        "\n",
        "flan_output = FLAN_query({\n",
        "\t\"inputs\": \"does ethanol take more energy make that produces\\nFalse\\ndo iran and afghanistan speak the same language\\nTrue\\nis elder scrolls online the same as skyrim\\n\",\n",
        "})\n",
        "\n",
        "gpt_output = GPT_query({\n",
        "\t\"inputs\": \"does ethanol take more energy make that produces\\nFalse\\ndo iran and afghanistan speak the same language\\nTrue\\nhave the milwaukee bucks ever won a championship\\n\",\n",
        "})\n",
        "\n",
        "print(bart_output)\n",
        "print(flan_output)\n",
        "#print(gpt_output[0][\"generated_text\"][len(\"does ethanol take more energy make that produces\\nFalse\\ndo iran and afghanistan speak the same language\\nTrue\\nhave the milwaukee bucks ever won a championship\\n\"):])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5gXKIS2BF__",
        "outputId": "fcefaa73-e1b0-4ecd-ec1b-85c6cd779c84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sequence': 'The Empire State Building in NYC is taller than Mount Everest.', 'labels': ['true', 'false'], 'scores': [0.9543874263763428, 0.045612581074237823]}\n",
            "[{'generated_text': 'False'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main methods used to query the models.\n",
        "\n",
        "def generate_bart_results(question, answer):\n",
        "  bart_output = BART_query(\n",
        "      {\n",
        "          \"inputs\": question,\n",
        "          \"parameters\": {\"candidate_labels\": [\"true\",\"false\"]}, # Specify the labels\n",
        "      }\n",
        "  )\n",
        "\n",
        "  # If there's an error in the response, we most likely hit the rate limit.\n",
        "  if 'error' in bart_output:\n",
        "    return None\n",
        "\n",
        "  # Set the label that had the higher probability.\n",
        "  output_label = 1 if bart_output[\"scores\"][0] > bart_output[\"scores\"][1] else 0\n",
        "\n",
        "  # Record prompt in a text file.\n",
        "  f = open(\"bart_results.txt\",\"a+\")\n",
        "  f.write(f\"Question: {question}. Provided Answer: {output_label}. Correct Answer: {answer}\\n\")\n",
        "  f.close()\n",
        "\n",
        "  return output_label\n",
        "\n",
        "def generate_flan_results(question, title, passage, answer, title_used = False):\n",
        "  # Generate the prompts used. For this method (default), we used two-shot (two examples) before the actual prompt.\n",
        "\n",
        "  # Different prompts if we want a title.\n",
        "  if title_used:\n",
        "    few_shot_str = \"Check (chess)\\nIn informal games, it is customary to announce ``check'' when making a move that puts the opponent's king in check. In formal competitions, however, check is rarely announced.\\ndo you always have to say check in chess\\nFalse\\n\"\n",
        "    few_shot_str += \"Jack Russell Terrier\\nThe Jack Russell Terrier is a small terrier that has its origins in fox hunting. It is principally white-bodied and smooth, rough or broken-coated but can be any colour.\\nis a jack russell considered a small breed?\\nTrue\\n\"\n",
        "    query_str = f\"{title}\\n{passage}\\n{question}\\n\"\n",
        "  else:\n",
        "    few_shot_str = \"In informal games, it is customary to announce ``check'' when making a move that puts the opponent's king in check. In formal competitions, however, check is rarely announced.\\ndo you always have to say check in chess\\nFalse\\n\"\n",
        "    few_shot_str += \"The Jack Russell Terrier is a small terrier that has its origins in fox hunting. It is principally white-bodied and smooth, rough or broken-coated but can be any colour.\\nis a jack russell considered a small breed?\\nTrue\\n\"\n",
        "    query_str = f\"{passage}\\n{question}\\n\"\n",
        "\n",
        "  # Query the model\n",
        "  flan_output = FLAN_query({\n",
        "    \"inputs\": few_shot_str + query_str,\n",
        "    \"parameters\": {\n",
        "        \"return_full_text\": False,\n",
        "        \"do_sample\": False,\n",
        "        \"max_new_tokens\": 5\n",
        "    }\n",
        "  })\n",
        "\n",
        "  # If there's an error in the response, we most likely hit the rate limit.\n",
        "  if 'error' in flan_output:\n",
        "    print(\"FLAN ERROR\" + str(title_used))\n",
        "    return flan_output[\"error\"]\n",
        "\n",
        "  # Get the completed text from the model and set it to 1,0,2. (2 being a non boolean answer.)\n",
        "  flan_output = flan_output[0][\"generated_text\"].lower()\n",
        "  if \"true\" in flan_output or \"yes\" in flan_output:\n",
        "    output_label = 1\n",
        "  elif \"false\" in flan_output or \"no\" in flan_output:\n",
        "    output_label = 0\n",
        "  else:\n",
        "    output_label = 2\n",
        "\n",
        "  # Record results.\n",
        "  if title_used:\n",
        "    f = open(\"flan_results_with_title.txt\",\"a+\")\n",
        "  else:\n",
        "    f = open(\"flan_results_no_title.txt\",\"a+\")\n",
        "  f.write(f\"Question: {question}. Provided Answer: {output_label}. Correct Answer: {answer}\\n\")\n",
        "  f.close()\n",
        "\n",
        "  return output_label\n",
        "\n",
        "def generate_gpt_results(question, title, passage, answer, title_used = False):\n",
        "  # Generate the prompts used. For this method (default), we used two-shot (two examples) before the actual prompt.\n",
        "\n",
        "  # Different prompts if we want a title.\n",
        "  if title_used:\n",
        "    few_shot_str = \"Check (chess)\\nIn informal games, it is customary to announce ``check'' when making a move that puts the opponent's king in check. In formal competitions, however, check is rarely announced.\\ndo you always have to say check in chess\\nFalse\\n\"\n",
        "    few_shot_str += \"Jack Russell Terrier\\nThe Jack Russell Terrier is a small terrier that has its origins in fox hunting. It is principally white-bodied and smooth, rough or broken-coated but can be any colour.\\nis a jack russell considered a small breed?\\nTrue\\n\"\n",
        "    query_str = f\"{title}\\n{passage}\\n{question}\\n\"\n",
        "  else:\n",
        "    few_shot_str = \"In informal games, it is customary to announce ``check'' when making a move that puts the opponent's king in check. In formal competitions, however, check is rarely announced.\\ndo you always have to say check in chess\\nFalse\\n\"\n",
        "    few_shot_str += \"The Jack Russell Terrier is a small terrier that has its origins in fox hunting. It is principally white-bodied and smooth, rough or broken-coated but can be any colour.\\nis a jack russell considered a small breed?\\nTrue\\n\"\n",
        "    query_str = f\"{passage}\\n{question}\\nTrue or False: \"\n",
        "\n",
        "  # Query the model\n",
        "  gpt_output = GPT_query({\n",
        "    \"inputs\": few_shot_str + query_str,\n",
        "    \"parameters\": {\n",
        "        \"return_full_text\": False,\n",
        "        \"do_sample\": False,\n",
        "        \"max_new_tokens\": 5\n",
        "    }\n",
        "  })\n",
        "  if 'error' in gpt_output:\n",
        "    print(\"GPT ERROR\" + str(title_used))\n",
        "    return None\n",
        "  gpt_output = gpt_output[0][\"generated_text\"].lower()\n",
        "  if \"true\" in gpt_output or \"yes\" in gpt_output:\n",
        "    output_label = 1\n",
        "  elif \"false\" in gpt_output or \"no\" in gpt_output:\n",
        "    output_label = 0\n",
        "  else:\n",
        "    output_label = 2\n",
        "\n",
        "  # Get the completed text from the model and set it to 1,0,2. (2 being a non boolean answer.)\n",
        "  if title_used:\n",
        "    f = open(\"gpt_results_with_title.txt\",\"a+\")\n",
        "  else:\n",
        "    f = open(\"gpt_results_no_title.txt\",\"a+\")\n",
        "  f.write(f\"Question: {question}. Provided Answer: {output_label}. Correct Answer: {answer}\\n\")\n",
        "  f.close()\n",
        "\n",
        "  return output_label\n",
        "\n"
      ],
      "metadata": {
        "id": "R_k-sbvzCyqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = [0,1,0,0,0,1,1] \n",
        "y_test = [1,0,1,1,0,1,0]\n",
        "\n",
        "# Example of how we calculated the scores.\n",
        "precision, recall, fscore, support = score(y_test, predicted, average = 'binary', zero_division = 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHRtaYJcDuEy",
        "outputId": "8a1d4653-f1f1-4447-ebe2-aa2203e55404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First version of this cell that the lower cells are refering to. This is the cell that actually tests and scores the models. (The Experiments)\n",
        "\n",
        "import time\n",
        "\n",
        "test_questions_answers = []\n",
        "\n",
        "bart_predictions = []\n",
        "flan_with_title_predictions = []\n",
        "gpt_with_title_predictions = []\n",
        "flan_no_title_predictions = []\n",
        "gpt_no_title_predictions = []\n",
        "answers = []\n",
        "count = 0\n",
        "\n",
        "# Iterate through a sub-section of the data set, pass through each model, record results, and then calculate scores.\n",
        "# We skipped an iteration if any of the models returned an error (most likely a rate limit) to keep the amount of\n",
        "# results the same for all models (for score calculation as well)\n",
        "with open('dev.jsonl', 'r') as data_set:\n",
        "  data_json = list(data_set)\n",
        "  data_json = data_json[300:400] #maybe randomize as well\n",
        "  for json_entry in data_json:\n",
        "    json_entry = json.loads(json_entry)\n",
        "    question = json_entry[\"question\"]\n",
        "    title = json_entry[\"title\"]\n",
        "    answer = 1 if json_entry[\"answer\"] == True else 0\n",
        "    passage = json_entry[\"passage\"]\n",
        "\n",
        "    bart_prediction = generate_bart_results(question, answer)\n",
        "    if bart_prediction == None:\n",
        "      #print(1)\n",
        "      continue\n",
        "    \n",
        "    time.sleep(2)\n",
        "\n",
        "    flan_with_title_prediction = generate_flan_results(question, title, passage, answer, title_used = True)\n",
        "    if flan_with_title_prediction == None:\n",
        "      #print(2)\n",
        "      continue\n",
        "    \n",
        "    time.sleep(2)\n",
        "\n",
        "    flan_no_title_prediction = generate_flan_results(question, title, passage, answer, title_used = False)\n",
        "    if flan_no_title_prediction == None:\n",
        "      #print(3)\n",
        "      continue\n",
        "\n",
        "    time.sleep(5)\n",
        "\n",
        "    gpt_with_title_prediction = generate_gpt_results(question, title, passage, answer, title_used = True)\n",
        "    if gpt_with_title_prediction == None:\n",
        "      #print(4)\n",
        "      continue\n",
        "\n",
        "    time.sleep(2)\n",
        "\n",
        "    gpt_no_title_prediction = generate_gpt_results(question, title, passage, answer, title_used = False)\n",
        "    if gpt_no_title_prediction == None:\n",
        "      #print(5)\n",
        "      continue\n",
        "    \n",
        "    time.sleep(2)\n",
        "\n",
        "    gpt_no_title_predictions.append(gpt_no_title_prediction)\n",
        "    bart_predictions.append(bart_prediction)\n",
        "    flan_with_title_predictions.append(flan_with_title_prediction)\n",
        "    flan_no_title_predictions.append(flan_no_title_prediction)\n",
        "    gpt_with_title_predictions.append(gpt_with_title_prediction)\n",
        "    answers.append(answer)\n",
        "    count += 1\n",
        "\n",
        "# Basic print statements\n",
        "print(count)\n",
        "print(bart_predictions)\n",
        "print(flan_with_title_predictions)\n",
        "print(flan_no_title_predictions)\n",
        "print(gpt_with_title_predictions)\n",
        "print(flan_no_title_predictions)\n",
        "print(answers)\n",
        "\n",
        "# Calculate scores for each model/experiment.\n",
        "bart_scores = score(bart_predictions, answers, average = 'binary', zero_division = 0)\n",
        "flan_with_title_scores = score(flan_with_title_predictions, answers, average = 'weighted', zero_division = 0)\n",
        "flan_no_title_scores = score(flan_no_title_predictions, answers, average = 'weighted', zero_division = 0)\n",
        "gpt_with_title_scores = score(gpt_with_title_predictions, answers, average = 'weighted', zero_division = 0)\n",
        "gpt_no_title_scores = score(gpt_no_title_predictions, answers, average = 'weighted', zero_division = 0)\n",
        "\n",
        "# Format the scores\n",
        "BART_Scores_Formatted = f\"BART Scores: Precision: {bart_scores[0]}, Recall: {bart_scores[1]}, F1: {bart_scores[2]}\\n\"\n",
        "FLAN_WITH_TITLE_Scores_Formatted = f\"FLAN With Title Scores: Precision: {flan_with_title_scores[0]}, Recall: {flan_with_title_scores[1]}, F1: {flan_with_title_scores[2]}\\n\"\n",
        "FLAN_NO_TITLE_Scores_Formatted = f\"FLAN With No Title Scores: Precision: {flan_no_title_scores[0]}, Recall: {flan_no_title_scores[1]}, F1: {flan_no_title_scores[2]}\\n\"\n",
        "GPT_WITH_TITLE_Scores_Formatted = f\"GPT With Title Scores: Precision: {gpt_with_title_scores[0]}, Recall: {gpt_with_title_scores[1]}, F1: {gpt_with_title_scores[2]}\\n\"\n",
        "GPT_NO_TITLE_Scores_Formatted = f\"GPT With No Title Scores: Precision: {gpt_no_title_scores[0]}, Recall: {gpt_no_title_scores[1]}, F1: {gpt_no_title_scores[2]}\\n\"\n",
        "\n",
        "# Print and store the scores into a text file.\n",
        "print(f\"Count: {str(count)} \\n\")\n",
        "print(BART_Scores_Formatted)\n",
        "print(FLAN_WITH_TITLE_Scores_Formatted)\n",
        "print(FLAN_NO_TITLE_Scores_Formatted)\n",
        "print(GPT_WITH_TITLE_Scores_Formatted)\n",
        "print(GPT_NO_TITLE_Scores_Formatted)\n",
        "f = open(\"Score_Results.txt\",\"w+\")\n",
        "f.write(f\"Count: {str(count)} \\n\")\n",
        "f.write(BART_Scores_Formatted + \"\\n\")\n",
        "f.write(FLAN_WITH_TITLE_Scores_Formatted + \"\\n\")\n",
        "f.write(FLAN_NO_TITLE_Scores_Formatted + \"\\n\")\n",
        "f.write(GPT_WITH_TITLE_Scores_Formatted + \"\\n\")\n",
        "f.write(GPT_NO_TITLE_Scores_Formatted + \"\\n\")\n",
        "f.close()\n"
      ],
      "metadata": {
        "id": "sz3-1m3kNiEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2c3f1bc-ffb7-4a3a-d591-a666f671a4e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT ERRORTrue\n",
            "GPT ERRORTrue\n",
            "GPT ERRORTrue\n",
            "GPT ERRORTrue\n",
            "GPT ERRORTrue\n",
            "GPT ERRORTrue\n",
            "GPT ERRORFalse\n",
            "GPT ERRORTrue\n",
            "GPT ERRORTrue\n",
            "GPT ERRORTrue\n",
            "GPT ERRORFalse\n",
            "72\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "[1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "[1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "[2, 2, 2, 1, 0, 2, 1, 1, 0, 2, 1, 1, 1, 0, 2, 2, 1, 0, 2, 2, 0, 2, 2, 0, 2, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 0, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2]\n",
            "[1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "[1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Count: 72 \n",
            "\n",
            "BART Scores: Precision: 1.0, Recall: 0.7083333333333334, F1: 0.8292682926829268\n",
            "\n",
            "FLAN With Title Scores: Precision: 0.8876439464674759, Recall: 0.8888888888888888, F1: 0.8874747474747475\n",
            "\n",
            "FLAN With No Title Scores: Precision: 0.9018829754123873, Recall: 0.9027777777777778, F1: 0.9021285849515185\n",
            "\n",
            "GPT With Title Scores: Precision: 0.1263616557734205, Recall: 0.3055555555555556, F1: 0.17846270928462712\n",
            "\n",
            "GPT With No Title Scores: Precision: 0.04481792717086834, Recall: 0.16666666666666666, F1: 0.07007575757575757\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment with FLAN (Check general/default methods above for comments.)\n",
        "\n",
        "def generate_flan_results_one_example(question, title, passage, answer, title_used = False):\n",
        "  if title_used:\n",
        "    few_shot_str = \"Check (chess)\\nIn informal games, it is customary to announce ``check'' when making a move that puts the opponent's king in check. In formal competitions, however, check is rarely announced.\\ndo you always have to say check in chess\\nFalse\\n\"\n",
        "    query_str = f\"{title}\\n{passage}\\n{question}\\n\"\n",
        "  else:\n",
        "    few_shot_str = \"In informal games, it is customary to announce ``check'' when making a move that puts the opponent's king in check. In formal competitions, however, check is rarely announced.\\ndo you always have to say check in chess\\nFalse\\n\"\n",
        "    query_str = f\"{passage}\\n{question}\\n\"\n",
        "  flan_output = FLAN_query({\n",
        "    \"inputs\": few_shot_str + query_str,\n",
        "    \"parameters\": {\n",
        "        \"return_full_text\": False,\n",
        "        \"do_sample\": False,\n",
        "        \"max_new_tokens\": 5\n",
        "    }\n",
        "  })\n",
        "\n",
        "  if 'error' in flan_output:\n",
        "    print(\"FLAN ERROR\" + str(title_used))\n",
        "    return flan_output[\"error\"]\n",
        "\n",
        "  flan_output = flan_output[0][\"generated_text\"].lower()\n",
        "  if \"true\" in flan_output or \"yes\" in flan_output:\n",
        "    output_label = 1\n",
        "  elif \"false\" in flan_output or \"no\" in flan_output:\n",
        "    output_label = 0\n",
        "  else:\n",
        "    output_label = 2\n",
        "\n",
        "  if title_used:\n",
        "    f = open(\"flan_results_with_title_one_example.txt\",\"a+\")\n",
        "  else:\n",
        "    f = open(\"flan_results_no_title_one_example.txt\",\"a+\")\n",
        "  f.write(f\"Question: {question}. Provided Answer: {output_label}. Correct Answer: {answer}\\n\")\n",
        "  f.close()\n",
        "\n",
        "  return output_label"
      ],
      "metadata": {
        "id": "dD4u3KhnWAUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check first version of this cell above for comments/documentation.\n",
        "\n",
        "import time\n",
        "\n",
        "flan_with_title_predictions = []\n",
        "flan_no_title_predictions = []\n",
        "answers = []\n",
        "count = 0\n",
        "\n",
        "with open('dev.jsonl', 'r') as data_set:\n",
        "  data_json = list(data_set)\n",
        "  data_json = data_json[300:400] #maybe randomize as well\n",
        "  for json_entry in data_json:\n",
        "    json_entry = json.loads(json_entry)\n",
        "    question = json_entry[\"question\"]\n",
        "    title = json_entry[\"title\"]\n",
        "    answer = 1 if json_entry[\"answer\"] == True else 0\n",
        "    passage = json_entry[\"passage\"]\n",
        "\n",
        "    flan_with_title_prediction = generate_flan_results_one_example(question, title, passage, answer, title_used = True)\n",
        "    if flan_with_title_prediction == None:\n",
        "      #print(2)\n",
        "      continue\n",
        "    \n",
        "    time.sleep(2)\n",
        "\n",
        "    flan_no_title_prediction = generate_flan_results_one_example(question, title, passage, answer, title_used = False)\n",
        "    if flan_no_title_prediction == None:\n",
        "      #print(3)\n",
        "      continue\n",
        "\n",
        "    time.sleep(2)\n",
        "\n",
        "    flan_with_title_predictions.append(flan_with_title_prediction)\n",
        "    flan_no_title_predictions.append(flan_no_title_prediction)\n",
        "    answers.append(answer)\n",
        "    count += 1\n",
        "\n",
        "print(flan_with_title_predictions)\n",
        "print(flan_no_title_predictions)\n",
        "print(answers)\n",
        "\n",
        "#precision, recall, fscore, support = score(y_test, predicted, average = 'binary', zero_division = 0)\n",
        "flan_with_title_scores = score(flan_with_title_predictions, answers, average = 'weighted', zero_division = 0)\n",
        "flan_no_title_scores = score(flan_no_title_predictions, answers, average = 'weighted', zero_division = 0)\n",
        "\n",
        "FLAN_WITH_TITLE_Scores_Formatted = f\"FLAN With Title Scores: Precision: {flan_with_title_scores[0]}, Recall: {flan_with_title_scores[1]}, F1: {flan_with_title_scores[2]}\\n\"\n",
        "FLAN_NO_TITLE_Scores_Formatted = f\"FLAN With No Title Scores: Precision: {flan_no_title_scores[0]}, Recall: {flan_no_title_scores[1]}, F1: {flan_no_title_scores[2]}\\n\"\n",
        "print(f\"Count: {str(count)} \\n\")\n",
        "print(FLAN_WITH_TITLE_Scores_Formatted)\n",
        "print(FLAN_NO_TITLE_Scores_Formatted)\n",
        "f = open(\"One_Example_Score_Results.txt\",\"w+\")\n",
        "f.write(f\"Count: {str(count)} \\n\")\n",
        "f.write(FLAN_WITH_TITLE_Scores_Formatted + \"\\n\")\n",
        "f.write(FLAN_NO_TITLE_Scores_Formatted + \"\\n\")\n",
        "f.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XjKI10YV5mA",
        "outputId": "52b571eb-7c9f-4581-f934-619648aa450c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1]\n",
            "[0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1]\n",
            "[0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1]\n",
            "Count: 100 \n",
            "\n",
            "FLAN With Title Scores: Precision: 0.8709558823529412, Recall: 0.88, F1: 0.8743554952510176\n",
            "\n",
            "FLAN With No Title Scores: Precision: 0.8709558823529412, Recall: 0.88, F1: 0.8743554952510176\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment with FLAN (Check general/default methods above for comments.)\n",
        "\n",
        "def generate_flan_results_no_context_one_example(question, title, passage, answer):\n",
        "\n",
        "  few_shot_str = \"do you always have to say check in chess\\nFalse\\n\"\n",
        "  query_str = f\"{question}\\n\"\n",
        "  flan_output = FLAN_query({\n",
        "    \"inputs\": few_shot_str + query_str,\n",
        "    \"parameters\": {\n",
        "        \"return_full_text\": False,\n",
        "        \"do_sample\": False,\n",
        "        \"max_new_tokens\": 5\n",
        "    }\n",
        "  })\n",
        "\n",
        "  if 'error' in flan_output:\n",
        "    return flan_output[\"error\"]\n",
        "\n",
        "  #output_label = 1 if flan_output[0][\"generated_text\"] == \"True\" else 0\n",
        "  flan_output = flan_output[0][\"generated_text\"].lower()\n",
        "  if \"true\" in flan_output or \"yes\" in flan_output:\n",
        "    output_label = 1\n",
        "  elif \"false\" in flan_output or \"no\" in flan_output:\n",
        "    output_label = 0\n",
        "  else:\n",
        "    output_label = 2\n",
        "\n",
        "  f = open(\"flan_results_no_context_one_example.txt\",\"a+\")\n",
        "  f.write(f\"Question: {question}. Provided Answer: {output_label}. Correct Answer: {answer}\\n\")\n",
        "  f.close()\n",
        "\n",
        "  #output_label = flan_output[0][\"generated_text\"]\n",
        "  return output_label"
      ],
      "metadata": {
        "id": "7mmkn6cJbD2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check first version of this cell above for comments/documentation.\n",
        "\n",
        "import time\n",
        "\n",
        "flan_no_context_predictions = []\n",
        "answers = []\n",
        "count = 0\n",
        "\n",
        "with open('dev.jsonl', 'r') as data_set:\n",
        "  data_json = list(data_set)\n",
        "  data_json = data_json[300:400] #maybe randomize as well\n",
        "  for json_entry in data_json:\n",
        "    json_entry = json.loads(json_entry)\n",
        "    question = json_entry[\"question\"]\n",
        "    title = json_entry[\"title\"]\n",
        "    answer = 1 if json_entry[\"answer\"] == True else 0\n",
        "    passage = json_entry[\"passage\"]\n",
        "\n",
        "    flan_no_context_prediction = generate_flan_results_no_context_one_example(question, title, passage, answer)\n",
        "    if flan_no_context_prediction == None:\n",
        "      #print(3)\n",
        "      continue\n",
        "\n",
        "    time.sleep(2)\n",
        "\n",
        "    flan_no_context_predictions.append(flan_no_context_prediction)\n",
        "    answers.append(answer)\n",
        "    count += 1\n",
        "\n",
        "print(flan_no_context_predictions)\n",
        "print(answers)\n",
        "\n",
        "flan_no_context_scores = score(flan_no_context_predictions, answers, average = 'weighted', zero_division = 0)\n",
        "\n",
        "FLAN_NO_CONTEXT_Scores_Formatted = f\"FLAN With No Context Scores: Precision: {flan_no_context_scores[0]}, Recall: {flan_no_context_scores[1]}, F1: {flan_no_context_scores[2]}\\n\"\n",
        "print(f\"Count: {str(count)} \\n\")\n",
        "print(FLAN_NO_CONTEXT_Scores_Formatted)\n",
        "f = open(\"One_Example_No_Context_Score_Results.txt\",\"w+\")\n",
        "f.write(f\"Count: {str(count)} \\n\")\n",
        "f.write(FLAN_NO_CONTEXT_Scores_Formatted + \"\\n\")\n",
        "f.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j50JdLJGcX95",
        "outputId": "fa017f75-717d-425e-ded0-7d5bd0190fd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 2, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]\n",
            "[0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1]\n",
            "Count: 100 \n",
            "\n",
            "FLAN With No Title Scores: Precision: 0.6263602941176472, Recall: 0.59, F1: 0.5755628730191397\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment with FLAN (Check general/default methods above for comments.)\n",
        "\n",
        "def generate_flan_results_four_example(question, title, passage, answer):\n",
        "  few_shot_str = \"In informal games, it is customary to announce ``check'' when making a move that puts the opponent's king in check. In formal competitions, however, check is rarely announced.\\ndo you always have to say check in chess\\nFalse\\n\"\n",
        "  few_shot_str += \"The Jack Russell Terrier is a small terrier that has its origins in fox hunting. It is principally white-bodied and smooth, rough or broken-coated but can be any colour.\\nis a jack russell considered a small breed?\\nTrue\\n\"\n",
        "  few_shot_str += \"Calcium carbide is a chemical compound with the chemical formula of CaC. Its main use industrially is in the production of acetylene and calcium cyanamide.\\ncalcium carbide cac2 is the raw material for the production of acetylene?\\nTrue\\n\"\n",
        "  few_shot_str += \"Creme eggs are available annually between 1 January and Easter Day. In the UK in the 1980s, Cadbury made Creme Eggs available year-round but sales dropped and they returned to seasonal availability.\\nyou can buy cadburys creme eggs all year round?False\\n\"\n",
        "  query_str = f\"{passage}\\n{question}\\n\"\n",
        "  flan_output = FLAN_query({\n",
        "    \"inputs\": few_shot_str + query_str,\n",
        "    \"parameters\": {\n",
        "        \"return_full_text\": False,\n",
        "        \"do_sample\": False,\n",
        "        \"max_new_tokens\": 5\n",
        "    }\n",
        "  })\n",
        "\n",
        "  if 'error' in flan_output:\n",
        "    return None\n",
        "\n",
        "  #output_label = 1 if flan_output[0][\"generated_text\"] == \"True\" else 0\n",
        "  flan_output = flan_output[0][\"generated_text\"].lower()\n",
        "  if \"true\" in flan_output or \"yes\" in flan_output:\n",
        "    output_label = 1\n",
        "  elif \"false\" in flan_output or \"no\" in flan_output:\n",
        "    output_label = 0\n",
        "  else:\n",
        "    output_label = 2\n",
        "\n",
        "  f = open(\"flan_results_no_title_four_example.txt\",\"a+\")\n",
        "  f.write(f\"Question: {question}. Provided Answer: {output_label}. Correct Answer: {answer}\\n\")\n",
        "  f.close()\n",
        "\n",
        "  #output_label = flan_output[0][\"generated_text\"]\n",
        "  return output_label"
      ],
      "metadata": {
        "id": "wFtFrhiWwrpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check first version of this cell above for comments/documentation.\n",
        "\n",
        "import time\n",
        "\n",
        "flan_four_example_predictions = []\n",
        "answers = []\n",
        "count = 0\n",
        "\n",
        "with open('dev.jsonl', 'r') as data_set:\n",
        "  data_json = list(data_set)\n",
        "  data_json = data_json[300:400] #maybe randomize as well\n",
        "  for json_entry in data_json:\n",
        "    json_entry = json.loads(json_entry)\n",
        "    question = json_entry[\"question\"]\n",
        "    title = json_entry[\"title\"]\n",
        "    answer = 1 if json_entry[\"answer\"] == True else 0\n",
        "    passage = json_entry[\"passage\"]\n",
        "\n",
        "    flan_four_example_prediction = generate_flan_results_four_example(question, title, passage, answer)\n",
        "    if flan_four_example_prediction == None:\n",
        "      continue\n",
        "\n",
        "    time.sleep(2)\n",
        "\n",
        "    flan_four_example_predictions.append(flan_four_example_prediction)\n",
        "    answers.append(answer)\n",
        "    count += 1\n",
        "\n",
        "print(flan_four_example_predictions)\n",
        "print(answers)\n",
        "\n",
        "flan_four_example_scores = score(flan_four_example_predictions, answers, average = 'weighted', zero_division = 0)\n",
        "\n",
        "FLAN_FOUR_EXAMPLES_Scores_Formatted = f\"FLAN With Four Examples Scores: Precision: {flan_four_example_scores[0]}, Recall: {flan_four_example_scores[1]}, F1: {flan_four_example_scores[2]}\\n\"\n",
        "print(f\"Count: {str(count)} \\n\")\n",
        "print(FLAN_FOUR_EXAMPLES_Scores_Formatted)\n",
        "f = open(\"Four_Examples_FLAN_Score_Results.txt\",\"w+\")\n",
        "f.write(f\"Count: {str(count)} \\n\")\n",
        "f.write(FLAN_FOUR_EXAMPLES_Scores_Formatted + \"\\n\")\n",
        "f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znTJr0MszSGv",
        "outputId": "813196a5-f639-4488-dde2-deffeb7d3f1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1]\n",
            "[0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1]\n",
            "Count: 100 \n",
            "\n",
            "FLAN With Four Examples Scores: Precision: 0.8893382352941177, Recall: 0.89, F1: 0.8887779149365953\n",
            "\n"
          ]
        }
      ]
    }
  ]
}